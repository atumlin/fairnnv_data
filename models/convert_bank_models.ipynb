{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to process Adult Census data, edit/train models, and perform adversarial debiasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary libraries for the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:10:46.356964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 15:10:47.236003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.utils import to_categorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.onnx\n",
    "from scipy.io import savemat\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bank():\n",
    "    # Define paths and column names\n",
    "    file_path = '../data/bank/bank-additional-full.csv'\n",
    "    column_names = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', \n",
    "                    'month', 'day_of_week', 'duration', 'emp.var.rate', 'campaign', 'pdays', 'previous', 'poutcome', 'y']\n",
    "    na_values = ['unknown']\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path, sep=';', na_values=na_values)\n",
    "    \n",
    "    # Drop na values\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "    for col in categorical_features:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "    \n",
    "    # Binary transformation for age (1 if age >= 25, else 0)\n",
    "    df['age'] = df['age'].apply(lambda x: 1 if x >= 25 else 0)\n",
    "    \n",
    "    # Convert target variable to binary\n",
    "    df['y'] = df['y'].apply(lambda x: 1 if x == 'yes' else 0)\n",
    "    \n",
    "    # Select columns to keep (including the target variable 'y')\n",
    "    df = df[column_names]\n",
    "    \n",
    "    # Split features and labels\n",
    "    X = df.drop('y', axis=1)\n",
    "    y = df['y']\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Extract the protected attribute ('age' for demonstration, adjust as needed)\n",
    "    protected_attribute = X[:, df.columns.get_loc('age')]\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test, protected_train, protected_test = train_test_split(\n",
    "        X, to_categorical(y, num_classes=2), protected_attribute, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, protected_train, protected_test\n",
    "\n",
    "# Saves data for use in verification\n",
    "def load_and_save_bank_data():\n",
    "    X_train, X_test, y_train, y_test, _, _ = load_bank()\n",
    "    \n",
    "    # Scaling numerical features with MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # Prepare data dictionary to save as .mat file\n",
    "    data_dict = {\n",
    "        'X': X_test, \n",
    "        'y': y_test   \n",
    "    }\n",
    "    \n",
    "    # Save to .mat file for use in MATLAB\n",
    "    savemat(\"./processed_data/bank_data.mat\", data_dict)\n",
    "    print(\"Data saved to bank_data.mat\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Editing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to save the models as onnx files for verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the model as ONNX format\n",
    "def save_model_onnx(model, input_shape, onnx_file_path):\n",
    "    # Create a dummy input tensor with the correct input shape (batch_size, input_shape)\n",
    "    dummy_input = tf.random.normal([1] + list(input_shape))\n",
    "\n",
    "    # Convert the model to ONNX\n",
    "    model_proto, external_tensor_storage = tf2onnx.convert.from_keras(model, \n",
    "                                                                      input_signature=(tf.TensorSpec(shape=[None] + list(input_shape), dtype=tf.float32),),\n",
    "                                                                      opset=13)\n",
    "    \n",
    "    # Save the ONNX model to the specified path\n",
    "    with open(onnx_file_path, \"wb\") as f:\n",
    "        f.write(model_proto.SerializeToString())\n",
    "    \n",
    "    print(f\"Model has been saved in ONNX format at {onnx_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the models so they are able to be used in FairNNV. FairNNV cannot handle sigmoid so shift to softmax and adjust final layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Function to modify a model for multiclass classification\n",
    "def modify_model_for_multiclass(model_path, num_classes):\n",
    "    # with warnings.catch_warnings():\n",
    "    #     warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Create a new input layer with the correct shape\n",
    "    new_input = tf.keras.layers.Input(shape=(16,))\n",
    "    x = new_input\n",
    "\n",
    "    # Transfer the layers except the last one\n",
    "    for layer in model.layers[:-1]:\n",
    "        x = layer(x)\n",
    "\n",
    "    # Create a new output layer\n",
    "    output = tf.keras.layers.Dense(num_classes, activation='softmax', name='new_output')(x)\n",
    "    \n",
    "    # Create a new model\n",
    "    new_model = tf.keras.models.Model(inputs=new_input, outputs=output)\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "# Ensure the save directories exist\n",
    "model_dir = './bank/bank_h5'\n",
    "save_dir = './bank/bank_keras'\n",
    "onnx_save_dir = './bank/bank_onnx'\n",
    "num_classes = 2\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if not os.path.exists(onnx_save_dir):\n",
    "    os.makedirs(onnx_save_dir)\n",
    "\n",
    "# Modify each model in the directory to remove sigmoid\n",
    "for model_file in os.listdir(model_dir):\n",
    "    if model_file.endswith('.h5'):\n",
    "        model_path = os.path.join(model_dir, model_file)\n",
    "        new_model = modify_model_for_multiclass(model_path, num_classes)\n",
    "        \n",
    "        # Update the model's loss function\n",
    "        new_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        # Save the modified model\n",
    "        save_path = os.path.join(save_dir, model_file.replace('.h5', '.keras'))\n",
    "        new_model.save(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-train models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to bank_data.mat\n",
      "Loading model BM-7.keras\n",
      "Training model BM-7.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8619 - loss: 0.3433 - val_accuracy: 0.8784 - val_loss: 0.3261\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8806 - loss: 0.3161 - val_accuracy: 0.8831 - val_loss: 0.2977\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8929 - loss: 0.2735 - val_accuracy: 0.8872 - val_loss: 0.2645\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8927 - loss: 0.2519 - val_accuracy: 0.8913 - val_loss: 0.2511\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9012 - loss: 0.2335 - val_accuracy: 0.8899 - val_loss: 0.2492\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8999 - loss: 0.2295 - val_accuracy: 0.8899 - val_loss: 0.2496\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9009 - loss: 0.2272 - val_accuracy: 0.8879 - val_loss: 0.2556\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9005 - loss: 0.2262 - val_accuracy: 0.8932 - val_loss: 0.2480\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9017 - loss: 0.2247 - val_accuracy: 0.8891 - val_loss: 0.2425\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8998 - loss: 0.2270 - val_accuracy: 0.8883 - val_loss: 0.2435\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9043 - loss: 0.2158 - val_accuracy: 0.8913 - val_loss: 0.2404\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9006 - loss: 0.2202 - val_accuracy: 0.8913 - val_loss: 0.2386\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9034 - loss: 0.2152 - val_accuracy: 0.8899 - val_loss: 0.2465\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9016 - loss: 0.2158 - val_accuracy: 0.8916 - val_loss: 0.2382\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9013 - loss: 0.2177 - val_accuracy: 0.8934 - val_loss: 0.2394\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9030 - loss: 0.2132 - val_accuracy: 0.8891 - val_loss: 0.2439\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9004 - loss: 0.2201 - val_accuracy: 0.8903 - val_loss: 0.2384\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9014 - loss: 0.2172 - val_accuracy: 0.8893 - val_loss: 0.2401\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9028 - loss: 0.2133 - val_accuracy: 0.8936 - val_loss: 0.2372\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9025 - loss: 0.2128 - val_accuracy: 0.8924 - val_loss: 0.2364\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9005 - loss: 0.2197 - val_accuracy: 0.8938 - val_loss: 0.2368\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9023 - loss: 0.2144 - val_accuracy: 0.8924 - val_loss: 0.2379\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9044 - loss: 0.2109 - val_accuracy: 0.8924 - val_loss: 0.2363\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9076 - loss: 0.2082 - val_accuracy: 0.8909 - val_loss: 0.2386\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9036 - loss: 0.2128 - val_accuracy: 0.8909 - val_loss: 0.2375\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9052 - loss: 0.2124 - val_accuracy: 0.8938 - val_loss: 0.2358\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9021 - loss: 0.2125 - val_accuracy: 0.8819 - val_loss: 0.2469\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9031 - loss: 0.2087 - val_accuracy: 0.8893 - val_loss: 0.2393\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9062 - loss: 0.2109 - val_accuracy: 0.8924 - val_loss: 0.2362\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9064 - loss: 0.2059 - val_accuracy: 0.8899 - val_loss: 0.2365\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9056 - loss: 0.2113 - val_accuracy: 0.8864 - val_loss: 0.2393\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9012 - loss: 0.2105 - val_accuracy: 0.8883 - val_loss: 0.2409\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9087 - loss: 0.2060 - val_accuracy: 0.8903 - val_loss: 0.2400\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9063 - loss: 0.2059 - val_accuracy: 0.8872 - val_loss: 0.2381\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9075 - loss: 0.2023 - val_accuracy: 0.8897 - val_loss: 0.2372\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9020 - loss: 0.2144 - val_accuracy: 0.8940 - val_loss: 0.2346\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9054 - loss: 0.2066 - val_accuracy: 0.8885 - val_loss: 0.2398\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9087 - loss: 0.2015 - val_accuracy: 0.8930 - val_loss: 0.2376\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9037 - loss: 0.2124 - val_accuracy: 0.8852 - val_loss: 0.2387\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9059 - loss: 0.2067 - val_accuracy: 0.8895 - val_loss: 0.2390\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9106 - loss: 0.2035 - val_accuracy: 0.8766 - val_loss: 0.2531\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9063 - loss: 0.2088 - val_accuracy: 0.8926 - val_loss: 0.2343\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9080 - loss: 0.2023 - val_accuracy: 0.8872 - val_loss: 0.2364\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9080 - loss: 0.2016 - val_accuracy: 0.8911 - val_loss: 0.2356\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9049 - loss: 0.2074 - val_accuracy: 0.8838 - val_loss: 0.2419\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9055 - loss: 0.2068 - val_accuracy: 0.8887 - val_loss: 0.2375\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9071 - loss: 0.2085 - val_accuracy: 0.8909 - val_loss: 0.2397\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9077 - loss: 0.1999 - val_accuracy: 0.8860 - val_loss: 0.2426\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9078 - loss: 0.1990 - val_accuracy: 0.8897 - val_loss: 0.2357\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9110 - loss: 0.1953 - val_accuracy: 0.8842 - val_loss: 0.2405\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step\n",
      "Model BM-7.keras - Accuracy: 0.889963922597573\n",
      "Model BM-7.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-7.onnx\n",
      "Loading model BM-6.keras\n",
      "Training model BM-6.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:53:01.129250: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:53:01.129352: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:53:01.151177: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:53:01.151320: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8723 - loss: 0.5038 - val_accuracy: 0.8713 - val_loss: 0.3396\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8722 - loss: 0.3325 - val_accuracy: 0.8713 - val_loss: 0.3324\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8757 - loss: 0.3247 - val_accuracy: 0.8694 - val_loss: 0.3265\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8731 - loss: 0.3227 - val_accuracy: 0.8700 - val_loss: 0.3235\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8742 - loss: 0.3179 - val_accuracy: 0.8774 - val_loss: 0.3171\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8789 - loss: 0.3156 - val_accuracy: 0.8784 - val_loss: 0.3127\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8824 - loss: 0.3060 - val_accuracy: 0.8823 - val_loss: 0.3095\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8844 - loss: 0.3022 - val_accuracy: 0.8829 - val_loss: 0.3069\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8878 - loss: 0.2951 - val_accuracy: 0.8817 - val_loss: 0.3045\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8865 - loss: 0.2951 - val_accuracy: 0.8834 - val_loss: 0.3008\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8896 - loss: 0.2865 - val_accuracy: 0.8860 - val_loss: 0.2829\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8937 - loss: 0.2651 - val_accuracy: 0.8858 - val_loss: 0.2750\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8892 - loss: 0.2616 - val_accuracy: 0.8831 - val_loss: 0.2649\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8928 - loss: 0.2510 - val_accuracy: 0.8829 - val_loss: 0.2634\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.2446 - val_accuracy: 0.8827 - val_loss: 0.2598\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8892 - loss: 0.2514 - val_accuracy: 0.8827 - val_loss: 0.2600\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8862 - loss: 0.2493 - val_accuracy: 0.8817 - val_loss: 0.2581\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8913 - loss: 0.2463 - val_accuracy: 0.8825 - val_loss: 0.2575\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8938 - loss: 0.2382 - val_accuracy: 0.8827 - val_loss: 0.2565\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8923 - loss: 0.2389 - val_accuracy: 0.8838 - val_loss: 0.2574\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8882 - loss: 0.2445 - val_accuracy: 0.8838 - val_loss: 0.2559\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8931 - loss: 0.2397 - val_accuracy: 0.8842 - val_loss: 0.2553\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8917 - loss: 0.2390 - val_accuracy: 0.8848 - val_loss: 0.2552\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8936 - loss: 0.2403 - val_accuracy: 0.8852 - val_loss: 0.2546\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8930 - loss: 0.2384 - val_accuracy: 0.8854 - val_loss: 0.2545\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8928 - loss: 0.2426 - val_accuracy: 0.8852 - val_loss: 0.2557\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8936 - loss: 0.2383 - val_accuracy: 0.8848 - val_loss: 0.2560\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8908 - loss: 0.2463 - val_accuracy: 0.8870 - val_loss: 0.2535\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8934 - loss: 0.2373 - val_accuracy: 0.8852 - val_loss: 0.2551\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8899 - loss: 0.2434 - val_accuracy: 0.8877 - val_loss: 0.2529\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2338 - val_accuracy: 0.8877 - val_loss: 0.2528\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8925 - loss: 0.2356 - val_accuracy: 0.8848 - val_loss: 0.2581\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8940 - loss: 0.2376 - val_accuracy: 0.8875 - val_loss: 0.2523\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8935 - loss: 0.2388 - val_accuracy: 0.8862 - val_loss: 0.2548\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8969 - loss: 0.2358 - val_accuracy: 0.8877 - val_loss: 0.2523\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8952 - loss: 0.2379 - val_accuracy: 0.8877 - val_loss: 0.2516\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8963 - loss: 0.2357 - val_accuracy: 0.8852 - val_loss: 0.2574\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8955 - loss: 0.2352 - val_accuracy: 0.8883 - val_loss: 0.2523\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8962 - loss: 0.2331 - val_accuracy: 0.8885 - val_loss: 0.2511\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8941 - loss: 0.2408 - val_accuracy: 0.8883 - val_loss: 0.2513\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8959 - loss: 0.2348 - val_accuracy: 0.8854 - val_loss: 0.2544\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8944 - loss: 0.2373 - val_accuracy: 0.8887 - val_loss: 0.2506\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8978 - loss: 0.2349 - val_accuracy: 0.8893 - val_loss: 0.2507\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8986 - loss: 0.2335 - val_accuracy: 0.8848 - val_loss: 0.2546\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8972 - loss: 0.2305 - val_accuracy: 0.8872 - val_loss: 0.2533\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8999 - loss: 0.2331 - val_accuracy: 0.8881 - val_loss: 0.2520\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8972 - loss: 0.2311 - val_accuracy: 0.8879 - val_loss: 0.2522\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8977 - loss: 0.2296 - val_accuracy: 0.8868 - val_loss: 0.2512\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8893 - loss: 0.2422 - val_accuracy: 0.8891 - val_loss: 0.2499\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9021 - loss: 0.2265 - val_accuracy: 0.8887 - val_loss: 0.2494\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step\n",
      "Model BM-6.keras - Accuracy: 0.8881600524762218\n",
      "Model BM-6.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-6.onnx\n",
      "Loading model BM-5.keras\n",
      "Training model BM-5.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:53:36.744539: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:53:36.744705: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:53:36.770696: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:53:36.770839: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7303 - loss: 0.7644 - val_accuracy: 0.8713 - val_loss: 0.3808\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8776 - loss: 0.3630 - val_accuracy: 0.8713 - val_loss: 0.3613\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8778 - loss: 0.3453 - val_accuracy: 0.8713 - val_loss: 0.3444\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8765 - loss: 0.3310 - val_accuracy: 0.8713 - val_loss: 0.3256\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8746 - loss: 0.3126 - val_accuracy: 0.8741 - val_loss: 0.3082\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8837 - loss: 0.2930 - val_accuracy: 0.8825 - val_loss: 0.2903\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8888 - loss: 0.2766 - val_accuracy: 0.8848 - val_loss: 0.2705\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8943 - loss: 0.2523 - val_accuracy: 0.8864 - val_loss: 0.2543\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8947 - loss: 0.2369 - val_accuracy: 0.8864 - val_loss: 0.2495\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9003 - loss: 0.2291 - val_accuracy: 0.8829 - val_loss: 0.2530\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8941 - loss: 0.2346 - val_accuracy: 0.8854 - val_loss: 0.2449\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8982 - loss: 0.2225 - val_accuracy: 0.8848 - val_loss: 0.2441\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8994 - loss: 0.2247 - val_accuracy: 0.8862 - val_loss: 0.2446\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8989 - loss: 0.2219 - val_accuracy: 0.8866 - val_loss: 0.2432\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8952 - loss: 0.2290 - val_accuracy: 0.8860 - val_loss: 0.2433\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9004 - loss: 0.2222 - val_accuracy: 0.8838 - val_loss: 0.2437\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2232 - val_accuracy: 0.8877 - val_loss: 0.2427\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8980 - loss: 0.2253 - val_accuracy: 0.8864 - val_loss: 0.2416\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9013 - loss: 0.2236 - val_accuracy: 0.8872 - val_loss: 0.2419\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8960 - loss: 0.2280 - val_accuracy: 0.8883 - val_loss: 0.2422\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8982 - loss: 0.2250 - val_accuracy: 0.8860 - val_loss: 0.2413\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8945 - loss: 0.2315 - val_accuracy: 0.8854 - val_loss: 0.2414\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8962 - loss: 0.2291 - val_accuracy: 0.8895 - val_loss: 0.2414\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8984 - loss: 0.2290 - val_accuracy: 0.8885 - val_loss: 0.2413\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2210 - val_accuracy: 0.8866 - val_loss: 0.2402\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8987 - loss: 0.2196 - val_accuracy: 0.8877 - val_loss: 0.2417\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8977 - loss: 0.2265 - val_accuracy: 0.8848 - val_loss: 0.2438\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8967 - loss: 0.2254 - val_accuracy: 0.8889 - val_loss: 0.2414\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9015 - loss: 0.2181 - val_accuracy: 0.8877 - val_loss: 0.2405\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8994 - loss: 0.2206 - val_accuracy: 0.8881 - val_loss: 0.2403\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8978 - loss: 0.2259 - val_accuracy: 0.8864 - val_loss: 0.2420\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9011 - loss: 0.2193 - val_accuracy: 0.8887 - val_loss: 0.2401\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2246 - val_accuracy: 0.8879 - val_loss: 0.2399\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2175 - val_accuracy: 0.8887 - val_loss: 0.2405\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2189 - val_accuracy: 0.8868 - val_loss: 0.2397\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8987 - loss: 0.2245 - val_accuracy: 0.8866 - val_loss: 0.2398\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9022 - loss: 0.2156 - val_accuracy: 0.8870 - val_loss: 0.2403\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2212 - val_accuracy: 0.8883 - val_loss: 0.2404\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2250 - val_accuracy: 0.8870 - val_loss: 0.2396\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8976 - loss: 0.2228 - val_accuracy: 0.8864 - val_loss: 0.2431\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9015 - loss: 0.2171 - val_accuracy: 0.8875 - val_loss: 0.2397\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9011 - loss: 0.2218 - val_accuracy: 0.8877 - val_loss: 0.2395\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9015 - loss: 0.2169 - val_accuracy: 0.8883 - val_loss: 0.2404\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9027 - loss: 0.2187 - val_accuracy: 0.8875 - val_loss: 0.2394\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8979 - loss: 0.2243 - val_accuracy: 0.8872 - val_loss: 0.2409\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2242 - val_accuracy: 0.8887 - val_loss: 0.2439\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9006 - loss: 0.2224 - val_accuracy: 0.8883 - val_loss: 0.2403\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8976 - loss: 0.2272 - val_accuracy: 0.8870 - val_loss: 0.2421\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8989 - loss: 0.2212 - val_accuracy: 0.8868 - val_loss: 0.2393\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8976 - loss: 0.2279 - val_accuracy: 0.8879 - val_loss: 0.2396\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888us/step\n",
      "Model BM-5.keras - Accuracy: 0.8906198753689735\n",
      "Model BM-5.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-5.onnx\n",
      "Loading model BM-8.keras\n",
      "Training model BM-8.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:54:12.864168: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:54:12.864261: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:54:12.894691: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:54:12.894814: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8771 - loss: 1.0317 - val_accuracy: 0.8713 - val_loss: 0.3451\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8774 - loss: 0.3322 - val_accuracy: 0.8713 - val_loss: 0.3326\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8742 - loss: 0.3233 - val_accuracy: 0.8713 - val_loss: 0.3221\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8757 - loss: 0.3064 - val_accuracy: 0.8713 - val_loss: 0.3087\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8722 - loss: 0.2992 - val_accuracy: 0.8854 - val_loss: 0.2847\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8931 - loss: 0.2616 - val_accuracy: 0.8875 - val_loss: 0.2542\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8960 - loss: 0.2319 - val_accuracy: 0.8879 - val_loss: 0.2487\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8959 - loss: 0.2379 - val_accuracy: 0.8780 - val_loss: 0.2496\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8952 - loss: 0.2313 - val_accuracy: 0.8866 - val_loss: 0.2447\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8956 - loss: 0.2265 - val_accuracy: 0.8817 - val_loss: 0.2443\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8943 - loss: 0.2270 - val_accuracy: 0.8834 - val_loss: 0.2450\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8946 - loss: 0.2259 - val_accuracy: 0.8809 - val_loss: 0.2449\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8928 - loss: 0.2303 - val_accuracy: 0.8858 - val_loss: 0.2423\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8930 - loss: 0.2307 - val_accuracy: 0.8819 - val_loss: 0.2435\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8942 - loss: 0.2310 - val_accuracy: 0.8815 - val_loss: 0.2423\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8956 - loss: 0.2255 - val_accuracy: 0.8815 - val_loss: 0.2440\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8983 - loss: 0.2252 - val_accuracy: 0.8868 - val_loss: 0.2428\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8957 - loss: 0.2242 - val_accuracy: 0.8858 - val_loss: 0.2436\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8984 - loss: 0.2243 - val_accuracy: 0.8846 - val_loss: 0.2425\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2211 - val_accuracy: 0.8848 - val_loss: 0.2438\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8936 - loss: 0.2275 - val_accuracy: 0.8881 - val_loss: 0.2458\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8972 - loss: 0.2249 - val_accuracy: 0.8823 - val_loss: 0.2437\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8956 - loss: 0.2225 - val_accuracy: 0.8825 - val_loss: 0.2443\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8953 - loss: 0.2249 - val_accuracy: 0.8870 - val_loss: 0.2434\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8952 - loss: 0.2293 - val_accuracy: 0.8866 - val_loss: 0.2426\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8949 - loss: 0.2248 - val_accuracy: 0.8862 - val_loss: 0.2447\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8973 - loss: 0.2196 - val_accuracy: 0.8868 - val_loss: 0.2416\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8972 - loss: 0.2239 - val_accuracy: 0.8840 - val_loss: 0.2422\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8975 - loss: 0.2273 - val_accuracy: 0.8862 - val_loss: 0.2408\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9007 - loss: 0.2132 - val_accuracy: 0.8846 - val_loss: 0.2436\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9002 - loss: 0.2197 - val_accuracy: 0.8852 - val_loss: 0.2421\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8936 - loss: 0.2277 - val_accuracy: 0.8850 - val_loss: 0.2421\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8952 - loss: 0.2237 - val_accuracy: 0.8860 - val_loss: 0.2410\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8991 - loss: 0.2164 - val_accuracy: 0.8850 - val_loss: 0.2405\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8927 - loss: 0.2300 - val_accuracy: 0.8840 - val_loss: 0.2431\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8970 - loss: 0.2237 - val_accuracy: 0.8879 - val_loss: 0.2411\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8960 - loss: 0.2236 - val_accuracy: 0.8858 - val_loss: 0.2406\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8987 - loss: 0.2214 - val_accuracy: 0.8834 - val_loss: 0.2410\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8963 - loss: 0.2188 - val_accuracy: 0.8870 - val_loss: 0.2394\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8985 - loss: 0.2194 - val_accuracy: 0.8825 - val_loss: 0.2441\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9004 - loss: 0.2186 - val_accuracy: 0.8877 - val_loss: 0.2410\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8951 - loss: 0.2291 - val_accuracy: 0.8864 - val_loss: 0.2404\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8983 - loss: 0.2214 - val_accuracy: 0.8838 - val_loss: 0.2401\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8960 - loss: 0.2247 - val_accuracy: 0.8870 - val_loss: 0.2400\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8967 - loss: 0.2218 - val_accuracy: 0.8850 - val_loss: 0.2420\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8972 - loss: 0.2244 - val_accuracy: 0.8875 - val_loss: 0.2405\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8971 - loss: 0.2222 - val_accuracy: 0.8860 - val_loss: 0.2398\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8986 - loss: 0.2177 - val_accuracy: 0.8850 - val_loss: 0.2410\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9005 - loss: 0.2168 - val_accuracy: 0.8858 - val_loss: 0.2439\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8992 - loss: 0.2199 - val_accuracy: 0.8881 - val_loss: 0.2427\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Model BM-8.keras - Accuracy: 0.8916038045260741\n",
      "Model BM-8.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-8.onnx\n",
      "Loading model BM-4.keras\n",
      "Training model BM-4.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:54:56.512632: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:54:56.512773: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:54:56.551765: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:54:56.551870: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 18 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7391 - loss: 0.9212 - val_accuracy: 0.8764 - val_loss: 0.3294\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8815 - loss: 0.3146 - val_accuracy: 0.8850 - val_loss: 0.2902\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8936 - loss: 0.2596 - val_accuracy: 0.8885 - val_loss: 0.2588\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8968 - loss: 0.2383 - val_accuracy: 0.8875 - val_loss: 0.2618\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8979 - loss: 0.2318 - val_accuracy: 0.8899 - val_loss: 0.2494\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9003 - loss: 0.2248 - val_accuracy: 0.8918 - val_loss: 0.2430\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8985 - loss: 0.2307 - val_accuracy: 0.8905 - val_loss: 0.2438\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8946 - loss: 0.2271 - val_accuracy: 0.8911 - val_loss: 0.2416\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8973 - loss: 0.2245 - val_accuracy: 0.8897 - val_loss: 0.2376\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9042 - loss: 0.2151 - val_accuracy: 0.8881 - val_loss: 0.2404\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8992 - loss: 0.2236 - val_accuracy: 0.8922 - val_loss: 0.2430\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9036 - loss: 0.2161 - val_accuracy: 0.8926 - val_loss: 0.2336\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2174 - val_accuracy: 0.8916 - val_loss: 0.2372\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9031 - loss: 0.2144 - val_accuracy: 0.8930 - val_loss: 0.2358\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9032 - loss: 0.2158 - val_accuracy: 0.8920 - val_loss: 0.2370\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8994 - loss: 0.2190 - val_accuracy: 0.8854 - val_loss: 0.2361\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8978 - loss: 0.2193 - val_accuracy: 0.8807 - val_loss: 0.2418\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9015 - loss: 0.2135 - val_accuracy: 0.8891 - val_loss: 0.2335\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9046 - loss: 0.2090 - val_accuracy: 0.8870 - val_loss: 0.2377\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9026 - loss: 0.2139 - val_accuracy: 0.8913 - val_loss: 0.2324\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9011 - loss: 0.2140 - val_accuracy: 0.8860 - val_loss: 0.2332\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9006 - loss: 0.2156 - val_accuracy: 0.8868 - val_loss: 0.2357\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9053 - loss: 0.2117 - val_accuracy: 0.8827 - val_loss: 0.2365\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9075 - loss: 0.1991 - val_accuracy: 0.8926 - val_loss: 0.2375\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9053 - loss: 0.2061 - val_accuracy: 0.8846 - val_loss: 0.2332\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9051 - loss: 0.2094 - val_accuracy: 0.8895 - val_loss: 0.2313\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9051 - loss: 0.2062 - val_accuracy: 0.8856 - val_loss: 0.2374\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9027 - loss: 0.2107 - val_accuracy: 0.8889 - val_loss: 0.2692\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9053 - loss: 0.2081 - val_accuracy: 0.8879 - val_loss: 0.2344\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9050 - loss: 0.2079 - val_accuracy: 0.8879 - val_loss: 0.2313\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9070 - loss: 0.2023 - val_accuracy: 0.8856 - val_loss: 0.2311\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9050 - loss: 0.2077 - val_accuracy: 0.8885 - val_loss: 0.2343\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9063 - loss: 0.2068 - val_accuracy: 0.8909 - val_loss: 0.2319\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9040 - loss: 0.2096 - val_accuracy: 0.8918 - val_loss: 0.2306\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9085 - loss: 0.2006 - val_accuracy: 0.8897 - val_loss: 0.2488\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9060 - loss: 0.2029 - val_accuracy: 0.8883 - val_loss: 0.2356\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9021 - loss: 0.2086 - val_accuracy: 0.8932 - val_loss: 0.2316\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9046 - loss: 0.2025 - val_accuracy: 0.8889 - val_loss: 0.2299\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9028 - loss: 0.2029 - val_accuracy: 0.8940 - val_loss: 0.2297\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9055 - loss: 0.2038 - val_accuracy: 0.8895 - val_loss: 0.2293\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9049 - loss: 0.1992 - val_accuracy: 0.8942 - val_loss: 0.2336\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9062 - loss: 0.2017 - val_accuracy: 0.8940 - val_loss: 0.2376\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9066 - loss: 0.2009 - val_accuracy: 0.8918 - val_loss: 0.2331\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9072 - loss: 0.1993 - val_accuracy: 0.8895 - val_loss: 0.2341\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9060 - loss: 0.2013 - val_accuracy: 0.8918 - val_loss: 0.2326\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9051 - loss: 0.2007 - val_accuracy: 0.8899 - val_loss: 0.2436\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9050 - loss: 0.2029 - val_accuracy: 0.8928 - val_loss: 0.2315\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9100 - loss: 0.1967 - val_accuracy: 0.8795 - val_loss: 0.2404\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9072 - loss: 0.1942 - val_accuracy: 0.8854 - val_loss: 0.2349\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9069 - loss: 0.1957 - val_accuracy: 0.8881 - val_loss: 0.2357\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Model BM-4.keras - Accuracy: 0.8896359462118727\n",
      "Model BM-4.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-4.onnx\n",
      "Loading model BM-3.keras\n",
      "Training model BM-3.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:55:42.786432: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:55:42.786522: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:55:42.818322: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:55:42.818423: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 10 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.6164 - loss: 2.1814 - val_accuracy: 0.8768 - val_loss: 0.3054\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8895 - loss: 0.2856 - val_accuracy: 0.8854 - val_loss: 0.2757\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - accuracy: 0.8935 - loss: 0.2588 - val_accuracy: 0.8889 - val_loss: 0.2634\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8937 - loss: 0.2473 - val_accuracy: 0.8893 - val_loss: 0.2545\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8951 - loss: 0.2425 - val_accuracy: 0.8903 - val_loss: 0.2599\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8973 - loss: 0.2350 - val_accuracy: 0.8907 - val_loss: 0.2539\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8986 - loss: 0.2356 - val_accuracy: 0.8885 - val_loss: 0.2487\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8955 - loss: 0.2344 - val_accuracy: 0.8913 - val_loss: 0.2478\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - accuracy: 0.8997 - loss: 0.2303 - val_accuracy: 0.8901 - val_loss: 0.2460\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8999 - loss: 0.2292 - val_accuracy: 0.8891 - val_loss: 0.2557\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8981 - loss: 0.2256 - val_accuracy: 0.8916 - val_loss: 0.2426\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8995 - loss: 0.2229 - val_accuracy: 0.8903 - val_loss: 0.2504\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9001 - loss: 0.2263 - val_accuracy: 0.8893 - val_loss: 0.2451\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9038 - loss: 0.2192 - val_accuracy: 0.8913 - val_loss: 0.2435\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9020 - loss: 0.2208 - val_accuracy: 0.8866 - val_loss: 0.2452\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9022 - loss: 0.2196 - val_accuracy: 0.8924 - val_loss: 0.2385\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8983 - loss: 0.2241 - val_accuracy: 0.8916 - val_loss: 0.2400\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9041 - loss: 0.2181 - val_accuracy: 0.8899 - val_loss: 0.2374\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1000us/step - accuracy: 0.9031 - loss: 0.2142 - val_accuracy: 0.8946 - val_loss: 0.2374\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1000us/step - accuracy: 0.9008 - loss: 0.2182 - val_accuracy: 0.8916 - val_loss: 0.2364\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9067 - loss: 0.2122 - val_accuracy: 0.8881 - val_loss: 0.2405\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - accuracy: 0.9063 - loss: 0.2104 - val_accuracy: 0.8870 - val_loss: 0.2439\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - accuracy: 0.9016 - loss: 0.2200 - val_accuracy: 0.8907 - val_loss: 0.2391\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9012 - loss: 0.2175 - val_accuracy: 0.8930 - val_loss: 0.2361\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9038 - loss: 0.2164 - val_accuracy: 0.8920 - val_loss: 0.2357\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9055 - loss: 0.2092 - val_accuracy: 0.8891 - val_loss: 0.2357\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8999 - loss: 0.2162 - val_accuracy: 0.8895 - val_loss: 0.2357\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9035 - loss: 0.2138 - val_accuracy: 0.8866 - val_loss: 0.2404\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9003 - loss: 0.2163 - val_accuracy: 0.8916 - val_loss: 0.2334\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9059 - loss: 0.2106 - val_accuracy: 0.8928 - val_loss: 0.2343\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9050 - loss: 0.2075 - val_accuracy: 0.8916 - val_loss: 0.2347\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9063 - loss: 0.2080 - val_accuracy: 0.8913 - val_loss: 0.2373\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9011 - loss: 0.2159 - val_accuracy: 0.8870 - val_loss: 0.2389\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9051 - loss: 0.2113 - val_accuracy: 0.8891 - val_loss: 0.2398\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9031 - loss: 0.2133 - val_accuracy: 0.8932 - val_loss: 0.2354\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9076 - loss: 0.2089 - val_accuracy: 0.8893 - val_loss: 0.2397\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9022 - loss: 0.2159 - val_accuracy: 0.8852 - val_loss: 0.2447\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9053 - loss: 0.2088 - val_accuracy: 0.8901 - val_loss: 0.2374\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - accuracy: 0.9026 - loss: 0.2127 - val_accuracy: 0.8899 - val_loss: 0.2378\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9032 - loss: 0.2083 - val_accuracy: 0.8916 - val_loss: 0.2362\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9088 - loss: 0.2028 - val_accuracy: 0.8922 - val_loss: 0.2344\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9033 - loss: 0.2107 - val_accuracy: 0.8858 - val_loss: 0.2425\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9044 - loss: 0.2063 - val_accuracy: 0.8891 - val_loss: 0.2358\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9061 - loss: 0.2061 - val_accuracy: 0.8922 - val_loss: 0.2359\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9053 - loss: 0.2085 - val_accuracy: 0.8942 - val_loss: 0.2366\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2174 - val_accuracy: 0.8938 - val_loss: 0.2358\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9057 - loss: 0.2044 - val_accuracy: 0.8909 - val_loss: 0.2352\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9073 - loss: 0.2087 - val_accuracy: 0.8913 - val_loss: 0.2363\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9046 - loss: 0.2028 - val_accuracy: 0.8870 - val_loss: 0.2379\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9048 - loss: 0.2100 - val_accuracy: 0.8930 - val_loss: 0.2386\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step\n",
      "Model BM-3.keras - Accuracy: 0.8934076746474254\n",
      "Model BM-3.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-3.onnx\n",
      "Loading model BM-2.keras\n",
      "Training model BM-2.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:56:20.287058: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:56:20.287149: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:56:20.306438: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:56:20.306528: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8406 - loss: 0.6006 - val_accuracy: 0.8676 - val_loss: 0.3525\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8765 - loss: 0.3296 - val_accuracy: 0.8797 - val_loss: 0.2976\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8851 - loss: 0.2897 - val_accuracy: 0.8854 - val_loss: 0.2814\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8887 - loss: 0.2714 - val_accuracy: 0.8852 - val_loss: 0.2789\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8900 - loss: 0.2608 - val_accuracy: 0.8883 - val_loss: 0.2584\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8947 - loss: 0.2511 - val_accuracy: 0.8897 - val_loss: 0.2570\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2380 - val_accuracy: 0.8907 - val_loss: 0.2533\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8903 - loss: 0.2487 - val_accuracy: 0.8893 - val_loss: 0.2538\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8982 - loss: 0.2382 - val_accuracy: 0.8893 - val_loss: 0.2559\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8968 - loss: 0.2413 - val_accuracy: 0.8905 - val_loss: 0.2541\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8953 - loss: 0.2407 - val_accuracy: 0.8911 - val_loss: 0.2517\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8973 - loss: 0.2367 - val_accuracy: 0.8889 - val_loss: 0.2544\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8943 - loss: 0.2440 - val_accuracy: 0.8903 - val_loss: 0.2479\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8959 - loss: 0.2396 - val_accuracy: 0.8889 - val_loss: 0.2523\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9032 - loss: 0.2251 - val_accuracy: 0.8870 - val_loss: 0.2608\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8989 - loss: 0.2310 - val_accuracy: 0.8889 - val_loss: 0.2625\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8980 - loss: 0.2315 - val_accuracy: 0.8899 - val_loss: 0.2449\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2289 - val_accuracy: 0.8911 - val_loss: 0.2428\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8969 - loss: 0.2285 - val_accuracy: 0.8872 - val_loss: 0.2537\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8973 - loss: 0.2262 - val_accuracy: 0.8891 - val_loss: 0.2439\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8999 - loss: 0.2259 - val_accuracy: 0.8922 - val_loss: 0.2423\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8964 - loss: 0.2321 - val_accuracy: 0.8889 - val_loss: 0.2499\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9025 - loss: 0.2207 - val_accuracy: 0.8920 - val_loss: 0.2391\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9009 - loss: 0.2220 - val_accuracy: 0.8889 - val_loss: 0.2443\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8980 - loss: 0.2289 - val_accuracy: 0.8877 - val_loss: 0.2460\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2276 - val_accuracy: 0.8936 - val_loss: 0.2400\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9030 - loss: 0.2220 - val_accuracy: 0.8918 - val_loss: 0.2411\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9026 - loss: 0.2212 - val_accuracy: 0.8924 - val_loss: 0.2380\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9048 - loss: 0.2197 - val_accuracy: 0.8916 - val_loss: 0.2382\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9044 - loss: 0.2125 - val_accuracy: 0.8838 - val_loss: 0.2533\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9004 - loss: 0.2211 - val_accuracy: 0.8920 - val_loss: 0.2388\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9036 - loss: 0.2153 - val_accuracy: 0.8901 - val_loss: 0.2388\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9048 - loss: 0.2133 - val_accuracy: 0.8764 - val_loss: 0.2561\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8978 - loss: 0.2231 - val_accuracy: 0.8911 - val_loss: 0.2382\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9001 - loss: 0.2198 - val_accuracy: 0.8909 - val_loss: 0.2381\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2164 - val_accuracy: 0.8918 - val_loss: 0.2377\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9031 - loss: 0.2139 - val_accuracy: 0.8834 - val_loss: 0.2487\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9038 - loss: 0.2194 - val_accuracy: 0.8895 - val_loss: 0.2399\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8990 - loss: 0.2248 - val_accuracy: 0.8866 - val_loss: 0.2401\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9006 - loss: 0.2191 - val_accuracy: 0.8875 - val_loss: 0.2394\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2173 - val_accuracy: 0.8918 - val_loss: 0.2370\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9042 - loss: 0.2169 - val_accuracy: 0.8920 - val_loss: 0.2375\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9048 - loss: 0.2151 - val_accuracy: 0.8891 - val_loss: 0.2405\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8989 - loss: 0.2197 - val_accuracy: 0.8913 - val_loss: 0.2365\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9042 - loss: 0.2169 - val_accuracy: 0.8916 - val_loss: 0.2371\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8996 - loss: 0.2190 - val_accuracy: 0.8883 - val_loss: 0.2390\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9027 - loss: 0.2216 - val_accuracy: 0.8911 - val_loss: 0.2356\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9030 - loss: 0.2150 - val_accuracy: 0.8905 - val_loss: 0.2358\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9025 - loss: 0.2143 - val_accuracy: 0.8924 - val_loss: 0.2356\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9024 - loss: 0.2169 - val_accuracy: 0.8881 - val_loss: 0.2473\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step\n",
      "Model BM-2.keras - Accuracy: 0.8855362413906199\n",
      "Model BM-2.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-2.onnx\n",
      "Loading model BM-1.keras\n",
      "Training model BM-1.keras\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:56:57.261902: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:56:57.261989: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:56:57.282848: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:56:57.282965: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "/home/annemtumlin/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8654 - loss: 2.1468 - val_accuracy: 0.8692 - val_loss: 0.3443\n",
      "Epoch 2/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8722 - loss: 0.3309 - val_accuracy: 0.8711 - val_loss: 0.3337\n",
      "Epoch 3/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8762 - loss: 0.3210 - val_accuracy: 0.8729 - val_loss: 0.3282\n",
      "Epoch 4/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8829 - loss: 0.3106 - val_accuracy: 0.8745 - val_loss: 0.3195\n",
      "Epoch 5/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8797 - loss: 0.3073 - val_accuracy: 0.8793 - val_loss: 0.3069\n",
      "Epoch 6/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8880 - loss: 0.2880 - val_accuracy: 0.8838 - val_loss: 0.2779\n",
      "Epoch 7/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8877 - loss: 0.2678 - val_accuracy: 0.8881 - val_loss: 0.2565\n",
      "Epoch 8/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8930 - loss: 0.2405 - val_accuracy: 0.8823 - val_loss: 0.2551\n",
      "Epoch 9/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8972 - loss: 0.2325 - val_accuracy: 0.8848 - val_loss: 0.2455\n",
      "Epoch 10/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8993 - loss: 0.2251 - val_accuracy: 0.8860 - val_loss: 0.2446\n",
      "Epoch 11/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9036 - loss: 0.2173 - val_accuracy: 0.8852 - val_loss: 0.2435\n",
      "Epoch 12/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8943 - loss: 0.2297 - val_accuracy: 0.8858 - val_loss: 0.2427\n",
      "Epoch 13/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8986 - loss: 0.2244 - val_accuracy: 0.8866 - val_loss: 0.2455\n",
      "Epoch 14/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8956 - loss: 0.2296 - val_accuracy: 0.8844 - val_loss: 0.2420\n",
      "Epoch 15/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8925 - loss: 0.2361 - val_accuracy: 0.8860 - val_loss: 0.2423\n",
      "Epoch 16/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8964 - loss: 0.2296 - val_accuracy: 0.8829 - val_loss: 0.2436\n",
      "Epoch 17/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8959 - loss: 0.2270 - val_accuracy: 0.8807 - val_loss: 0.2440\n",
      "Epoch 18/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8966 - loss: 0.2232 - val_accuracy: 0.8799 - val_loss: 0.2469\n",
      "Epoch 19/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8975 - loss: 0.2248 - val_accuracy: 0.8895 - val_loss: 0.2439\n",
      "Epoch 20/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8978 - loss: 0.2248 - val_accuracy: 0.8893 - val_loss: 0.2428\n",
      "Epoch 21/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9009 - loss: 0.2215 - val_accuracy: 0.8870 - val_loss: 0.2400\n",
      "Epoch 22/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9006 - loss: 0.2202 - val_accuracy: 0.8811 - val_loss: 0.2438\n",
      "Epoch 23/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8977 - loss: 0.2234 - val_accuracy: 0.8836 - val_loss: 0.2422\n",
      "Epoch 24/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2225 - val_accuracy: 0.8881 - val_loss: 0.2402\n",
      "Epoch 25/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2217 - val_accuracy: 0.8875 - val_loss: 0.2401\n",
      "Epoch 26/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8955 - loss: 0.2253 - val_accuracy: 0.8885 - val_loss: 0.2402\n",
      "Epoch 27/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8981 - loss: 0.2259 - val_accuracy: 0.8860 - val_loss: 0.2400\n",
      "Epoch 28/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9006 - loss: 0.2201 - val_accuracy: 0.8881 - val_loss: 0.2400\n",
      "Epoch 29/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8999 - loss: 0.2210 - val_accuracy: 0.8872 - val_loss: 0.2396\n",
      "Epoch 30/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8972 - loss: 0.2205 - val_accuracy: 0.8877 - val_loss: 0.2404\n",
      "Epoch 31/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8974 - loss: 0.2269 - val_accuracy: 0.8930 - val_loss: 0.2443\n",
      "Epoch 32/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9003 - loss: 0.2242 - val_accuracy: 0.8850 - val_loss: 0.2423\n",
      "Epoch 33/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2221 - val_accuracy: 0.8875 - val_loss: 0.2393\n",
      "Epoch 34/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8986 - loss: 0.2217 - val_accuracy: 0.8922 - val_loss: 0.2403\n",
      "Epoch 35/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9007 - loss: 0.2228 - val_accuracy: 0.8887 - val_loss: 0.2398\n",
      "Epoch 36/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8997 - loss: 0.2305 - val_accuracy: 0.8881 - val_loss: 0.2392\n",
      "Epoch 37/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9020 - loss: 0.2186 - val_accuracy: 0.8899 - val_loss: 0.2413\n",
      "Epoch 38/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8985 - loss: 0.2234 - val_accuracy: 0.8866 - val_loss: 0.2394\n",
      "Epoch 39/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9029 - loss: 0.2152 - val_accuracy: 0.8881 - val_loss: 0.2396\n",
      "Epoch 40/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9069 - loss: 0.2122 - val_accuracy: 0.8870 - val_loss: 0.2389\n",
      "Epoch 41/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8997 - loss: 0.2206 - val_accuracy: 0.8889 - val_loss: 0.2387\n",
      "Epoch 42/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9029 - loss: 0.2168 - val_accuracy: 0.8905 - val_loss: 0.2386\n",
      "Epoch 43/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9068 - loss: 0.2168 - val_accuracy: 0.8866 - val_loss: 0.2392\n",
      "Epoch 44/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8975 - loss: 0.2252 - val_accuracy: 0.8907 - val_loss: 0.2398\n",
      "Epoch 45/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9061 - loss: 0.2147 - val_accuracy: 0.8856 - val_loss: 0.2405\n",
      "Epoch 46/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8998 - loss: 0.2228 - val_accuracy: 0.8893 - val_loss: 0.2392\n",
      "Epoch 47/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9004 - loss: 0.2208 - val_accuracy: 0.8875 - val_loss: 0.2391\n",
      "Epoch 48/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8991 - loss: 0.2230 - val_accuracy: 0.8864 - val_loss: 0.2384\n",
      "Epoch 49/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9021 - loss: 0.2171 - val_accuracy: 0.8885 - val_loss: 0.2390\n",
      "Epoch 50/50\n",
      "\u001b[1m610/610\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9017 - loss: 0.2207 - val_accuracy: 0.8877 - val_loss: 0.2389\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step\n",
      "Model BM-1.keras - Accuracy: 0.8907838635618236\n",
      "Model BM-1.keras retrained and saved successfully.\n",
      "Model has been saved in ONNX format at ./bank/bank_onnx/BM-1.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:57:35.946426: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:57:35.946526: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-07-19 15:57:35.970757: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "2024-07-19 15:57:35.970968: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the Bank dataset\n",
    "X_train, X_test, y_train, y_test = load_and_save_bank_data()\n",
    "\n",
    "for model_file in os.listdir(save_dir):\n",
    "    if model_file.endswith('.keras'):\n",
    "        model_path = os.path.join(save_dir, model_file)\n",
    "        \n",
    "        try:\n",
    "            # Load the modified model\n",
    "            print(f\"Loading model {model_file}\")\n",
    "            # with warnings.catch_warnings():\n",
    "            #     warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            model = load_model(model_path)\n",
    "\n",
    "            # Reinitialize the optimizer\n",
    "            model.compile(\n",
    "                optimizer=Adam(),\n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Fit the model\n",
    "            print(f\"Training model {model_file}\")\n",
    "            history = model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "            # Evaluate the model\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "            accuracy = accuracy_score(np.argmax(y_test, axis=1), y_pred_classes)\n",
    "\n",
    "            print(f\"Model {model_file} - Accuracy: {accuracy}\")\n",
    "\n",
    "            # Save the retrained model\n",
    "            model.save(model_path)\n",
    "            print(f\"Model {model_file} retrained and saved successfully.\")\n",
    "\n",
    "            # Save the model as ONNX\n",
    "            onnx_save_path = os.path.join(onnx_save_dir, model_file.replace('.keras', '.onnx'))\n",
    "            save_model_onnx(model, (16,), onnx_save_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {model_file}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversairal Debiasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To determine which adversarial debiasing framework to run to collect results\n",
    "multiple_runs = False\n",
    "singular_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save model metrics to csv\n",
    "def save_metrics_to_csv(filename, model_file, model_name, classification_accuracy, balanced_accuracy, disparate_impact, equal_opportunity_difference, average_odds_difference,precision,recall,f1):\n",
    "    # Check if the file exists to write the header only once\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            # Write the header if the file does not exist\n",
    "            writer.writerow(['Model File', 'Model', 'Classification Accuracy', 'Balanced Accuracy', 'Disparate Impact', 'Equal Opportunity Difference', 'Average Odds Difference', 'Precision', 'Recall','F1'])\n",
    "        \n",
    "        # Write the metrics\n",
    "        writer.writerow([model_file, model_name, classification_accuracy, balanced_accuracy, disparate_impact, equal_opportunity_difference, average_odds_difference, precision, recall, f1])\n",
    "\n",
    "# function to save the mean/std dev of model metrics for multiple runs to csv\n",
    "def save_metrics_to_csv_mr(filename, model_name, model_type, means, stds):\n",
    "    headers = [\n",
    "        'model_name', 'model_type', 'metric', 'mean', 'std_dev'\n",
    "    ]\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if os.path.getsize(filename) == 0:\n",
    "            writer.writerow(headers)\n",
    "        for metric, mean_value in means.items():\n",
    "            writer.writerow([model_name, model_type, metric, mean_value, stds[metric]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various metrics for evaluation including accuracy and fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Metrics calculation functions\n",
    "def precision(y_true, y_pred, average='binary'):\n",
    "    return precision_score(y_true, y_pred, average=average)\n",
    "\n",
    "def recall(y_true, y_pred, average='binary'):\n",
    "    return recall_score(y_true, y_pred, average=average)\n",
    "\n",
    "def f1(y_true, y_pred, average='binary'):\n",
    "    return f1_score(y_true, y_pred, average=average)\n",
    "\n",
    "def classification_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def balanced_accuracy(y_true, y_pred):\n",
    "    classes = np.unique(y_true)\n",
    "    recall_scores = []\n",
    "    for cls in classes:\n",
    "        true_positives = np.sum((y_true == cls) & (y_pred == cls))\n",
    "        possible_positives = np.sum(y_true == cls)\n",
    "        recall_scores.append(true_positives / possible_positives)\n",
    "    return np.mean(recall_scores)\n",
    "\n",
    "def disparate_impact(y_true, y_pred, protected_attribute):\n",
    "    privileged = protected_attribute == 1\n",
    "    unprivileged = protected_attribute == 0\n",
    "    if np.sum(privileged) == 0 or np.sum(unprivileged) == 0:\n",
    "        return np.nan\n",
    "    privileged_outcome = np.mean(y_pred[privileged]) if np.sum(privileged) > 0 else np.nan\n",
    "    unprivileged_outcome = np.mean(y_pred[unprivileged]) if np.sum(unprivileged) > 0 else np.nan\n",
    "    if privileged_outcome == 0:\n",
    "        return np.nan  \n",
    "    return unprivileged_outcome / privileged_outcome\n",
    "\n",
    "def equal_opportunity_difference(y_true, y_pred, protected_attribute):\n",
    "    privileged = protected_attribute == 1\n",
    "    unprivileged = protected_attribute == 0\n",
    "    true_positive_rate_privileged = np.sum((y_true[privileged] == 1) & (y_pred[privileged] == 1)) / np.sum(y_true[privileged] == 1)\n",
    "    true_positive_rate_unprivileged = np.sum((y_true[unprivileged] == 1) & (y_pred[unprivileged] == 1)) / np.sum(y_true[unprivileged] == 1)\n",
    "    return true_positive_rate_unprivileged - true_positive_rate_privileged\n",
    "\n",
    "def average_odds_difference(y_true, y_pred, protected_attribute):\n",
    "    privileged = protected_attribute == 1\n",
    "    unprivileged = protected_attribute == 0\n",
    "    tpr_privileged = np.sum((y_true[privileged] == 1) & (y_pred[privileged] == 1)) / np.sum(y_true[privileged] == 1)\n",
    "    tpr_unprivileged = np.sum((y_true[unprivileged] == 1) & (y_pred[unprivileged] == 1)) / np.sum(y_true[unprivileged] == 1)\n",
    "    fpr_privileged = np.sum((y_true[privileged] == 0) & (y_pred[privileged] == 1)) / np.sum(y_true[privileged] == 0)\n",
    "    fpr_unprivileged = np.sum((y_true[unprivileged] == 0) & (y_pred[unprivileged] == 1)) / np.sum(y_true[unprivileged] == 0)\n",
    "    average_odds_privileged = (tpr_privileged + fpr_privileged) / 2\n",
    "    average_odds_unprivileged = (tpr_unprivileged + fpr_unprivileged) / 2\n",
    "    return average_odds_unprivileged - average_odds_privileged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the adversarial model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversary model definition\n",
    "def build_adversary_model(input_shape):\n",
    "    adversary_input = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(64, activation='relu')(adversary_input)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    adversary_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    adversary_model = models.Model(inputs=adversary_input, outputs=adversary_output)\n",
    "    adversary_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return adversary_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to generate results on one run of the adversarial debiasing model. \n",
    "Also, there is provided functionality to view the accuracy/loss to evaluate overfitting versus underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 117\u001b[0m\n\u001b[1;32m    113\u001b[0m protected_batch \u001b[38;5;241m=\u001b[39m protected_train[batch_indices]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(persistent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Forward pass through the classifier\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     classifier_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Forward pass through the adversary with no gradient accumulation\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     adversary_predictions \u001b[38;5;241m=\u001b[39m adversary_model(classifier_predictions, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:110\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_traceback\u001b[39m(fn):\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filter out Keras-internal traceback frames in exceptions raised by fn.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_handler\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_traceback_filtering_enabled():\n\u001b[1;32m    113\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "if singular_run:\n",
    "    # Load and preprocess the data\n",
    "    X_train, X_test, y_train, y_test, protected_train, protected_test = load_bank()\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Directory paths\n",
    "    input_directory = './bank/bank_keras'\n",
    "    output_directory = './bank/bank_debiased_onnx'\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    metrics_filename = './model_metrics/bank_model_metrics.csv'\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "\n",
    "    # Iterate over all .keras files in the input directory to convert to ONNX file\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith('.keras'):\n",
    "            # Full path to the current model file\n",
    "            input_path = os.path.join(input_directory, file)\n",
    "            output_path = os.path.join(output_directory, file.replace('.keras', '.onnx'))\n",
    "\n",
    "            try:\n",
    "                # Load the model\n",
    "                print(f\"Loading model from {input_path}\")\n",
    "                classifier_model = load_model(input_path)\n",
    "\n",
    "                # Ensure the model is compiled with the correct optimizer and metrics\n",
    "                classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                # Print metrics for plain model\n",
    "                y_test_pred_plain = classifier_model.predict(X_test).argmax(axis=1)\n",
    "                y_test_true = y_test.argmax(axis=1)\n",
    "\n",
    "                plain_classification_accuracy = classification_accuracy(y_test_true, y_test_pred_plain)\n",
    "                plain_balanced_accuracy = balanced_accuracy(y_test_true, y_test_pred_plain)\n",
    "                plain_disparate_impact = disparate_impact(y_test_true, y_test_pred_plain, protected_test)\n",
    "                plain_equal_opportunity_difference = equal_opportunity_difference(y_test_true, y_test_pred_plain, protected_test)\n",
    "                plain_average_odds_difference = average_odds_difference(y_test_true, y_test_pred_plain, protected_test)\n",
    "                plain_precision = precision(y_test_true, y_test_pred_plain, average='macro')  # Use 'macro' for multi-class\n",
    "                plain_recall = recall(y_test_true, y_test_pred_plain, average='macro')        # Use 'macro' for multi-class\n",
    "                plain_f1 = f1(y_test_true, y_test_pred_plain, average='macro')                # Use 'macro' for multi-class\n",
    "\n",
    "                save_metrics_to_csv(metrics_filename, file, 'Plain Model', plain_classification_accuracy, plain_balanced_accuracy, plain_disparate_impact, plain_equal_opportunity_difference, plain_average_odds_difference, plain_precision, plain_recall, plain_f1)\n",
    "                \n",
    "                # Build and compile the adversary model\n",
    "                adversary_model = build_adversary_model(classifier_model.output_shape[1:])\n",
    "                \n",
    "                # Initialize lists to store metrics\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                train_accuracies = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                # Training parameters\n",
    "                num_epochs = 500\n",
    "                batch_size = 128\n",
    "                initial_learning_rate = 0.001\n",
    "                adversary_loss_weight_initial = 0.1\n",
    "                adversary_loss_weight_final = 0.7\n",
    "\n",
    "                # Optimizers\n",
    "                classifier_optimizer = tf.keras.optimizers.Adam(initial_learning_rate)\n",
    "                adversary_optimizer = tf.keras.optimizers.Adam(initial_learning_rate)\n",
    "\n",
    "                # Learning rate scheduler\n",
    "                lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate,\n",
    "                    decay_steps=10000,\n",
    "                    decay_rate=0.96,\n",
    "                    staircase=True\n",
    "                )\n",
    "\n",
    "                # Update the optimizer to use the learning rate scheduler\n",
    "                classifier_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "                adversary_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "                # Loss functions\n",
    "                classification_loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "                adversary_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "                # Training loop\n",
    "                best_val_loss = float('inf')\n",
    "                epochs_since_best = 0\n",
    "\n",
    "                # Training loop\n",
    "                for epoch in range(num_epochs):\n",
    "                    # Shuffle the training data\n",
    "                    indices = np.arange(X_train.shape[0])\n",
    "                    np.random.shuffle(indices)\n",
    "\n",
    "                    # Mini-batch training\n",
    "                    epoch_train_loss = 0\n",
    "                    epoch_train_accuracy = 0\n",
    "                    num_batches = 0\n",
    "                    for start in range(0, X_train.shape[0], batch_size):\n",
    "                        end = min(start + batch_size, X_train.shape[0])\n",
    "                        batch_indices = indices[start:end]\n",
    "\n",
    "                        X_batch = X_train[batch_indices]\n",
    "                        y_batch = y_train[batch_indices]\n",
    "                        protected_batch = protected_train[batch_indices].reshape(-1, 1)\n",
    "\n",
    "                        with tf.GradientTape(persistent=True) as tape:\n",
    "                            # Forward pass through the classifier\n",
    "                            classifier_predictions = classifier_model(X_batch, training=True)\n",
    "\n",
    "                            # Forward pass through the adversary with no gradient accumulation\n",
    "                            adversary_predictions = adversary_model(classifier_predictions, training=False)\n",
    "\n",
    "                            # Compute losses\n",
    "                            classification_loss = classification_loss_fn(y_batch, classifier_predictions)\n",
    "                            adversary_loss = adversary_loss_fn(protected_batch, adversary_predictions)\n",
    "\n",
    "                            # Linearly increase the adversary loss weight over epochs\n",
    "                            adversary_loss_weight = adversary_loss_weight_initial + \\\n",
    "                                                    (adversary_loss_weight_final - adversary_loss_weight_initial) * (epoch / num_epochs)\n",
    "\n",
    "                            total_loss = classification_loss - adversary_loss_weight * adversary_loss\n",
    "\n",
    "                        # Compute gradients and update classifier weights\n",
    "                        classifier_gradients = tape.gradient(total_loss, classifier_model.trainable_variables)\n",
    "                        classifier_optimizer.apply_gradients(zip(classifier_gradients, classifier_model.trainable_variables))\n",
    "\n",
    "                        with tape:\n",
    "                            # Forward pass through the classifier\n",
    "                            classifier_predictions = classifier_model(X_batch, training=True)\n",
    "\n",
    "                            # Forward pass through the adversary\n",
    "                            adversary_predictions = adversary_model(classifier_predictions, training=True)\n",
    "\n",
    "                            # Compute adversary loss\n",
    "                            adversary_loss = adversary_loss_fn(protected_batch, adversary_predictions)\n",
    "\n",
    "                        # Compute gradients and update adversary weights\n",
    "                        adversary_vars = [var for var in adversary_model.trainable_variables]\n",
    "                        adversary_gradients = tape.gradient(adversary_loss, adversary_vars)\n",
    "                        adversary_optimizer.apply_gradients(zip(adversary_gradients, adversary_vars))\n",
    "\n",
    "                        # Accumulate training loss and accuracy\n",
    "                        epoch_train_loss += classification_loss.numpy()\n",
    "                        epoch_train_accuracy += np.mean(np.argmax(classifier_predictions, axis=1) == np.argmax(y_batch, axis=1))\n",
    "                        num_batches += 1\n",
    "\n",
    "                    # Calculate average training loss and accuracy\n",
    "                    epoch_train_loss /= num_batches\n",
    "                    epoch_train_accuracy /= num_batches\n",
    "\n",
    "                    # Evaluate on validation set\n",
    "                    val_predictions = classifier_model.predict(X_test)\n",
    "                    val_loss = classification_loss_fn(y_test, val_predictions).numpy()\n",
    "                    val_accuracy = np.mean(np.argmax(val_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "                    # Store metrics\n",
    "                    train_losses.append(epoch_train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                    train_accuracies.append(epoch_train_accuracy)\n",
    "                    val_accuracies.append(val_accuracy)\n",
    "\n",
    "                    # Real-time plotting\n",
    "                    clear_output(wait=True)\n",
    "                    plt.figure(figsize=(14, 5))\n",
    "                    plt.subplot(1, 2, 1)\n",
    "                    plt.plot(train_losses, label='Training Loss')\n",
    "                    plt.plot(val_losses, label='Validation Loss')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Loss')\n",
    "                    plt.legend()\n",
    "                    plt.title('Training and Validation Loss')\n",
    "\n",
    "                    plt.subplot(1, 2, 2)\n",
    "                    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "                    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "                    plt.xlabel('Epochs')\n",
    "                    plt.ylabel('Accuracy')\n",
    "                    plt.legend()\n",
    "                    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "                    plt.show()\n",
    "\n",
    "                # print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_train_loss}, Validation Loss: {val_loss}, Train Accuracy: {epoch_train_accuracy}, Validation Accuracy: {val_accuracy}, Adversary Loss Weight: {adversary_loss_weight}\")\n",
    "\n",
    "                # Early stopping check\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        epochs_since_best = 0\n",
    "                    else:\n",
    "                        epochs_since_best += 1\n",
    "                        if epochs_since_best >= early_stopping.patience:\n",
    "                            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                            break\n",
    "\n",
    "                # Predictions for debiased model\n",
    "                y_test_pred_debiased = classifier_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "                debiased_classification_accuracy = classification_accuracy(y_test_true, y_test_pred_debiased)\n",
    "                debiased_balanced_accuracy = balanced_accuracy(y_test_true, y_test_pred_debiased)\n",
    "                debiased_disparate_impact = disparate_impact(y_test_true, y_test_pred_debiased, protected_test)\n",
    "                debiased_equal_opportunity_difference = equal_opportunity_difference(y_test_true, y_test_pred_debiased, protected_test)\n",
    "                debiased_average_odds_difference = average_odds_difference(y_test_true, y_test_pred_debiased, protected_test)\n",
    "                debiased_precision = precision(y_test_true, y_test_pred_plain, average='macro')  # Use 'macro' for multi-class\n",
    "                debiased_recall = recall(y_test_true, y_test_pred_plain, average='macro')        # Use 'macro' for multi-class\n",
    "                debiased_f1 = f1(y_test_true, y_test_pred_plain, average='macro')                # Use 'macro' for multi-class\n",
    "\n",
    "                save_metrics_to_csv(metrics_filename, file, 'Debiased Model', debiased_classification_accuracy, debiased_balanced_accuracy, debiased_disparate_impact, debiased_equal_opportunity_difference, debiased_average_odds_difference, debiased_precision, debiased_recall, debiased_f1)\n",
    "                \n",
    "                # Save the debiased model as ONNX\n",
    "                input_shape = (20,)  # Adjust the input shape based on your model's expected input\n",
    "                save_model_onnx(classifier_model, input_shape, output_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to convert {file}. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Debiasing Process For Multiple Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "if multiple_runs:\n",
    "    # Load and preprocess the data\n",
    "    X_train, X_test, y_train, y_test, protected_train, protected_test = load_bank()\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Directory paths\n",
    "    input_directory = './bank/bank_keras'\n",
    "    output_directory = './bank/bank_debiased_onnx'\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    metrics_filename = './model_metrics/bank_model_metrics_multiple_runs.csv'\n",
    "\n",
    "    # Number of runs\n",
    "    num_runs = 10\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True)\n",
    "\n",
    "    # Iterate over all .keras files in the input directory to convert to ONNX file\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith('.keras'):\n",
    "            # Full path to the current model file\n",
    "            input_path = os.path.join(input_directory, file)\n",
    "            output_path = os.path.join(output_directory, file.replace('.keras', '.onnx'))\n",
    "\n",
    "            try:\n",
    "                plain_metrics = {\n",
    "                    'classification_accuracy': [],\n",
    "                    'balanced_accuracy': [],\n",
    "                    'disparate_impact': [],\n",
    "                    'equal_opportunity_difference': [],\n",
    "                    'average_odds_difference': [],\n",
    "                    'precision': [],\n",
    "                    'recall': [],\n",
    "                    'f1': []\n",
    "                }\n",
    "\n",
    "                debiased_metrics = {\n",
    "                    'classification_accuracy': [],\n",
    "                    'balanced_accuracy': [],\n",
    "                    'disparate_impact': [],\n",
    "                    'equal_opportunity_difference': [],\n",
    "                    'average_odds_difference': [],\n",
    "                    'precision': [],\n",
    "                    'recall': [],\n",
    "                    'f1': []\n",
    "                }\n",
    "            \n",
    "                # Inside the loop for each .keras file\n",
    "                for run in range(num_runs):\n",
    "                    print(f\"Run {run + 1}/{num_runs}\")\n",
    "\n",
    "                    # Random seed for variability\n",
    "                    np.random.seed(run)\n",
    "                    tf.random.set_seed(run)\n",
    "\n",
    "                    # Shuffle the training data\n",
    "                    indices = np.arange(X_train.shape[0])\n",
    "                    np.random.shuffle(indices)\n",
    "                    X_train_shuffled = X_train[indices]\n",
    "                    y_train_shuffled = y_train[indices]\n",
    "                    protected_train_shuffled = protected_train[indices]\n",
    "\n",
    "                    # Load the model\n",
    "                    classifier_model = load_model(input_path)\n",
    "\n",
    "                    # Compile the model\n",
    "                    classifier_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                    # Train the model with shuffled data\n",
    "                    classifier_model.fit(X_train_shuffled, y_train_shuffled, epochs=10, batch_size=128, verbose=0)\n",
    "\n",
    "                    # Train and evaluate the plain model\n",
    "                    y_test_pred_plain = classifier_model.predict(X_test).argmax(axis=1)\n",
    "                    y_test_true = y_test.argmax(axis=1)\n",
    "\n",
    "                    plain_metrics['classification_accuracy'].append(classification_accuracy(y_test_true, y_test_pred_plain))\n",
    "                    plain_metrics['balanced_accuracy'].append(balanced_accuracy(y_test_true, y_test_pred_plain))\n",
    "                    plain_metrics['disparate_impact'].append(disparate_impact(y_test_true, y_test_pred_plain, protected_test))\n",
    "                    plain_metrics['equal_opportunity_difference'].append(equal_opportunity_difference(y_test_true, y_test_pred_plain, protected_test))\n",
    "                    plain_metrics['average_odds_difference'].append(average_odds_difference(y_test_true, y_test_pred_plain, protected_test))\n",
    "                    plain_metrics['precision'].append(precision_score(y_test_true, y_test_pred_plain, average='macro', zero_division=1))\n",
    "                    plain_metrics['recall'].append(recall_score(y_test_true, y_test_pred_plain, average='macro'))\n",
    "                    plain_metrics['f1'].append(f1_score(y_test_true, y_test_pred_plain, average='macro'))\n",
    "\n",
    "                    # Build and compile the adversary model\n",
    "                    adversary_model = build_adversary_model(classifier_model.output_shape[1:])\n",
    "\n",
    "                    # Training parameters\n",
    "                    num_epochs = 500\n",
    "                    batch_size = 128\n",
    "                    initial_learning_rate = 0.001\n",
    "                    adversary_loss_weight_initial = 0.1\n",
    "                    adversary_loss_weight_final = 0.7\n",
    "\n",
    "                    # Optimizers\n",
    "                    classifier_optimizer = tf.keras.optimizers.Adam(initial_learning_rate)\n",
    "                    adversary_optimizer = tf.keras.optimizers.Adam(initial_learning_rate)\n",
    "\n",
    "                    # Learning rate scheduler\n",
    "                    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                        initial_learning_rate,\n",
    "                        decay_steps=10000,\n",
    "                        decay_rate=0.96,\n",
    "                        staircase=True\n",
    "                    )\n",
    "\n",
    "                    # Update the optimizer to use the learning rate scheduler\n",
    "                    classifier_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "                    adversary_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "                    # Loss functions\n",
    "                    classification_loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "                    adversary_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "                    best_val_loss = float('inf')\n",
    "                    epochs_since_best = 0\n",
    "\n",
    "                    # Training loop\n",
    "                    for epoch in range(num_epochs):\n",
    "                        # Shuffle the training data\n",
    "                        indices = np.arange(X_train.shape[0])\n",
    "                        np.random.shuffle(indices)\n",
    "\n",
    "                        # Mini-batch training\n",
    "                        epoch_train_loss = 0\n",
    "                        epoch_train_accuracy = 0\n",
    "                        num_batches = 0\n",
    "                        for start in range(0, X_train.shape[0], batch_size):\n",
    "                            end = min(start + batch_size, X_train.shape[0])\n",
    "                            batch_indices = indices[start:end]\n",
    "\n",
    "                            X_batch = X_train[batch_indices]\n",
    "                            y_batch = y_train[batch_indices]\n",
    "                            protected_batch = protected_train[batch_indices].reshape(-1, 1)\n",
    "\n",
    "                            with tf.GradientTape(persistent=True) as tape:\n",
    "                                # Forward pass through the classifier\n",
    "                                classifier_predictions = classifier_model(X_batch, training=True)\n",
    "\n",
    "                                # Forward pass through the adversary with no gradient accumulation\n",
    "                                adversary_predictions = adversary_model(classifier_predictions, training=False)\n",
    "\n",
    "                                # Compute losses\n",
    "                                classification_loss = classification_loss_fn(y_batch, classifier_predictions)\n",
    "                                adversary_loss = adversary_loss_fn(protected_batch, adversary_predictions)\n",
    "\n",
    "                                # Linearly increase the adversary loss weight over epochs\n",
    "                                adversary_loss_weight = adversary_loss_weight_initial + \\\n",
    "                                                        (adversary_loss_weight_final - adversary_loss_weight_initial) * (epoch / num_epochs)\n",
    "\n",
    "                                total_loss = classification_loss - adversary_loss_weight * adversary_loss\n",
    "\n",
    "                            # Compute gradients and update classifier weights\n",
    "                            classifier_gradients = tape.gradient(total_loss, classifier_model.trainable_variables)\n",
    "                            classifier_optimizer.apply_gradients(zip(classifier_gradients, classifier_model.trainable_variables))\n",
    "\n",
    "                            with tape:\n",
    "                                # Forward pass through the classifier\n",
    "                                classifier_predictions = classifier_model(X_batch, training=True)\n",
    "\n",
    "                                # Forward pass through the adversary\n",
    "                                adversary_predictions = adversary_model(classifier_predictions, training=True)\n",
    "\n",
    "                                # Compute adversary loss\n",
    "                                adversary_loss = adversary_loss_fn(protected_batch, adversary_predictions)\n",
    "\n",
    "                            # Compute gradients and update adversary weights\n",
    "                            adversary_vars = [var for var in adversary_model.trainable_variables]\n",
    "                            adversary_gradients = tape.gradient(adversary_loss, adversary_vars)\n",
    "                            adversary_optimizer.apply_gradients(zip(adversary_gradients, adversary_vars))\n",
    "\n",
    "                            # Accumulate training loss and accuracy\n",
    "                            epoch_train_loss += classification_loss.numpy()\n",
    "                            epoch_train_accuracy += np.mean(np.argmax(classifier_predictions, axis=1) == np.argmax(y_batch, axis=1))\n",
    "                            num_batches += 1\n",
    "\n",
    "                        print(f\"Epoch {epoch + 1}/{num_epochs}, Classification Loss: {classification_loss.numpy()}, Adversary Loss: {adversary_loss.numpy()}\")\n",
    "\n",
    "                        # Calculate average training loss and accuracy\n",
    "                        epoch_train_loss /= num_batches\n",
    "                        epoch_train_accuracy /= num_batches\n",
    "\n",
    "                        # Evaluate on validation set\n",
    "                        val_predictions = classifier_model.predict(X_test)\n",
    "                        val_loss = classification_loss_fn(y_test, val_predictions).numpy()\n",
    "                        val_accuracy = np.mean(np.argmax(val_predictions, axis=1) == np.argmax(y_test, axis=1))\n",
    "\n",
    "                        # Early stopping check\n",
    "                        if val_loss < best_val_loss:\n",
    "                            best_val_loss = val_loss\n",
    "                            epochs_since_best = 0\n",
    "                        else:\n",
    "                            epochs_since_best += 1\n",
    "                            if epochs_since_best >= early_stopping.patience:\n",
    "                                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                                break\n",
    "\n",
    "                    y_test_pred_debiased = classifier_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "                    debiased_metrics['classification_accuracy'].append(classification_accuracy(y_test_true, y_test_pred_debiased))\n",
    "                    debiased_metrics['balanced_accuracy'].append(balanced_accuracy(y_test_true, y_test_pred_debiased))\n",
    "                    debiased_metrics['disparate_impact'].append(disparate_impact(y_test_true, y_test_pred_debiased, protected_test))\n",
    "                    debiased_metrics['equal_opportunity_difference'].append(equal_opportunity_difference(y_test_true, y_test_pred_debiased, protected_test))\n",
    "                    debiased_metrics['average_odds_difference'].append(average_odds_difference(y_test_true, y_test_pred_debiased, protected_test))\n",
    "                    debiased_metrics['precision'].append(precision_score(y_test_true, y_test_pred_debiased, average='macro', zero_division=1))\n",
    "                    debiased_metrics['recall'].append(recall_score(y_test_true, y_test_pred_debiased, average='macro'))\n",
    "                    debiased_metrics['f1'].append(f1_score(y_test_true, y_test_pred_debiased, average='macro'))\n",
    "\n",
    "                # Calculate mean and std for plain metrics\n",
    "                plain_means = {key: np.mean(values) for key, values in plain_metrics.items()}\n",
    "                plain_stds = {key: np.std(values) for key, values in plain_metrics.items()}\n",
    "\n",
    "                # Calculate mean and std for debiased metrics\n",
    "                debiased_means = {key: np.mean(values) for key, values in debiased_metrics.items()}\n",
    "                debiased_stds = {key: np.std(values) for key, values in debiased_metrics.items()}\n",
    "\n",
    "                # Save metrics to CSV\n",
    "                save_metrics_to_csv_mr(metrics_filename, file, 'Plain Model', plain_means, plain_stds)\n",
    "                save_metrics_to_csv_mr(metrics_filename, file, 'Debiased Model', debiased_means, debiased_stds)\n",
    "\n",
    "                # Save the debiased model as ONNX\n",
    "                input_shape = (20,)  # Adjust the input shape based on your model's expected input\n",
    "                save_model_onnx(classifier_model, input_shape, output_path)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to convert {file}. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
